---
title: "Predicting NBA shots"
output: rmarkdown::github_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE)
```

# Introduction
In this notebook, regression and machine learning techniques are used to predict
whether a basketball shot is succesful (hits the basket and scores a point) or not.
SPecifically, we'll look at all the basketball shots made during the 2014-2015 NBA 
season scraped from the NBA's API and provided on kaggle [(https://www.kaggle.com/dansbecker/nba-shot-logs/home)](https://www.kaggle.com/dansbecker/nba-shot-logs/home).

# Set up
```{r echo=T, results='hide', message=FALSE, warning=FALSE}
# Code which explores the NBA shot log data
# clear workspace
remove(list = ls())
# clear console
cat("\014")

# load packages
library(caret)
library(tidyverse)
library(skimr)
library(ggplot2)
library(tictoc)
library(corrplot)
library(pROC)
library(viridis) # for density plot
library(rpart.plot)

path_project <- "C:/Users/Mats Ole/Desktop/predicting_nba_shots/"
path_rel_data <- "data_input/"
name_data <- "shot_logs.csv"
```

# Data cleaning
First, let's clean and rename the data.
```{r}
df_raw <- read_csv(paste0(path_project, path_rel_data, name_data))

df_clean <- df_raw %>%
  transmute(game_id = as.factor(GAME_ID),
            matchup = as.factor(MATCHUP),
            home_game = case_when(LOCATION == "H" ~ TRUE,
                                  LOCATION == "A" ~ FALSE),
            win = case_when(W == "W" ~ TRUE,
                           W == "L" ~ FALSE),
            final_margin = abs(FINAL_MARGIN),
            shot_number = SHOT_NUMBER,
            period = PERIOD,
            game_clock = as.numeric(GAME_CLOCK),
            shot_clock = SHOT_CLOCK,
            dribbles = DRIBBLES,
            touch_time = TOUCH_TIME,
            shot_dist = SHOT_DIST,
            pts_type = as.factor(PTS_TYPE),
            shot_result = as.factor(SHOT_RESULT),
            closest_defender_name = as.factor(CLOSEST_DEFENDER),
            closest_defender_id = CLOSEST_DEFENDER_PLAYER_ID,
            closest_defender_dist = CLOSE_DEF_DIST,
            pts = PTS,
            player_name = as.factor(player_name),
            player_id = as.factor(player_id)
            ) %>% 
  dplyr::select(shot_result, everything())
```

Inspecting the data, we find that a feq observations have missing shot clock
values. To keep things simple and given how few observations are concerned,
we'll drop them.
```{r, results='asis'}
options(scipen=999)
skim_with(numeric = list(hist = NULL))
kable(skim(df_clean))

df_clean <- 
  df_clean %>% drop_na()
```

## Generating additional features
```{r echo=T}
df_high_pct_scorer <- 
  df_clean %>% 
  group_by(player_name) %>% 
  summarise(shot_percentage = mean(shot_result == "made"),
            number_shots_taken = n()) %>% 
  ungroup() %>% 
  filter(number_shots_taken >= 200) %>% 
  arrange(desc(shot_percentage)) %>% 
  top_n(20, shot_percentage)

df_low_pct_scorer <- 
  df_clean %>% 
  group_by(player_name) %>% 
  summarise(shot_percentage = mean(shot_result == "made"),
            number_shots_taken = n()) %>% 
  ungroup() %>% 
  filter(number_shots_taken >= 200) %>% 
  arrange(shot_percentage) %>% 
  top_n(-20, shot_percentage)

# df_most_frequent_shooters <- 
#   df_clean %>% 
#   group_by(player_name) %>% 
#   summarise(number_shots_taken = n()) %>% 
#   ungroup() %>% 
#   arrange(desc(number_shots_taken)) %>% 
#   top_n(5, number_shots_taken)
```

```{r echo=T}
df_best_defenders <- 
  df_clean %>% 
  group_by(closest_defender_name) %>% 
  summarise(opp_shot_percentage = mean(shot_result == "made"),
            number_shots_defended = n()) %>% 
  ungroup() %>% 
  filter(number_shots_defended >= 200) %>% 
  arrange(opp_shot_percentage) %>% 
  top_n(-20, opp_shot_percentage)

df_worst_defenders <- 
  df_clean %>% 
  group_by(closest_defender_name) %>% 
  summarise(opp_shot_percentage = mean(shot_result == "made"),
            number_shots_defended = n()) %>% 
  ungroup() %>% 
  filter(number_shots_defended >= 200) %>% 
  arrange(opp_shot_percentage) %>% 
  top_n(20, opp_shot_percentage)

# df_best_defenders <- 
#   df_clean %>% 
#   group_by(closest_defender) %>% 
#   summarise(shot_percentage = mean(shot_result == "made"),
#             number_shots_defended = n()) %>% 
#   ungroup() %>% 
#   filter(number_shots_defended >= 100) %>% 
#   arrange(shot_percentage)

```


```{r}
df_clean <- 
  df_clean %>% 
  mutate(
    high_pct_scorer = if_else(player_name %in% !! df_high_pct_scorer$player_name, TRUE, FALSE),
    low_pct_scorer = if_else(player_name %in% !! df_low_pct_scorer$player_name, TRUE, FALSE),
    high_pct_defender = if_else(closest_defender_name %in% !! df_best_defenders$closest_defender_name, TRUE, FALSE),
    low_pct_defender = if_else(closest_defender_name %in% !! df_worst_defenders$closest_defender_name, TRUE, FALSE)
  )
  

```

# Data exploration

## Overall shot percentage
45.6% of the shots are succesful.
```{r echo=T}
mean(df_clean$shot_result == "made")
```
## Most frequent shots are short-range 2-points shots or long-range 3-point shots.
```{r echo=T}

# Set display ranges to zoom in on areas where vast majority of data points lie 
display_range_x <- c(0, 30)
display_range_y <- c(0, 20)

ggplot(df_clean, 
       aes(x = shot_dist, 
           y = closest_defender_dist)
       ) +
  stat_density2d(aes(fill = ..density..), 
                 contour = F, geom = 'tile'
                 ) +
  scale_fill_viridis() + 
  ggtitle("Density distribution of succesful shots") +
  coord_cartesian(xlim = display_range_x, 
                  ylim = display_range_y
                  ) +
  geom_vline(xintercept = 22, 
             linetype = "dotted", 
             colour = "red") +
  geom_vline(xintercept = 23.75, 
             linetype = "dotted", 
             colour = "red") +
  geom_vline(xintercept = 23.75, 
             linetype = "dotted", 
             colour = "red") +
  annotate("text", 
           x = 21, 
           y = 15, 
           label = paste("Corner 3 points"), 
           size = 4, 
           angle = 90, 
           colour = "red") +
  annotate("text", 
           x = 24.75, 
           y = 15, 
           label = paste("Normal 3 points"), 
           size = 4, 
           angle = 90, 
           colour = "red")


```

## Short-range shots tend to be more succesful than long-range shots 
Below, I plot the difference in density between succesful and unsuccesful shots
in the shot-distance-closest-defender-space. The graph suggests that shot distance
and defender distance will play important rolls in classifying a shot succesful
or unsuccesful.
```{r}
library(mlbench)
library(rgl)
library(MASS)
library(plot3D)

df_made <- filter(df_clean, shot_result == "made")
df_missed <- filter(df_clean, shot_result == "missed")

n_grid <- 200

common_limits <- 
  c(range(df_clean$shot_dist), 
    range(df_clean$closest_defender_dist)
    )

kde_p <- kde2d(df_made$shot_dist,
               df_made$closest_defender_dist,
               n = n_grid, 
               lims = common_limits)

kde_n <- kde2d(df_missed$shot_dist,
               df_missed$closest_defender_dist,
               n = n_grid, 
               lims = common_limits)

z <- kde_p$z - kde_n$z

image2D(x = kde_p$x, 
        y = kde_p$y, 
        z,
        col = RColorBrewer::brewer.pal(11,"Spectral"),
        xlab = "Shot Distance",
        ylab = "Defender Distance",
        clab = "Density difference",
        shade = 0, 
        rasterImage = TRUE, 
        xlim = display_range_x, 
        ylim = display_range_y,
        contour = list(col = "white", 
                       labcex = 0.8, 
                       lwd = 1, 
                       alpha = 0.5)
        )

```

## Success rate in the shot-distance-closest-defender-space
```{r echo=T}

```


## Lebron James is an above-average shooter, but there is fat tail of high-percentage shooters that exceed his shooting percentage.
```{r echo=T}
df_best_shooters <- 
  df_clean %>% 
  group_by(player_name) %>% 
  summarise(shot_percentage = mean(shot_result == "made"),
            number_shots_taken = n()) %>% 
  ungroup() %>% 
  filter(number_shots_taken >= 100) %>% 
  arrange(desc(shot_percentage))

ggplot(df_best_shooters,
       aes(x = shot_percentage)) +
  geom_density() +
  geom_vline(xintercept = 0.494, linetype = "solid", colour = "red") +
  annotate("text", x = 0.5, y = 2.5, 
           label = paste("Lebron James, 947 shots taken"), 
           size = 4, angle = 90, colour = "red")
```
## The players with the highest shooting percentages tend to play on positions that are close to the basket.
```{r echo=T}
head(df_best_shooters)

df_best_defenders <- 
  df_clean %>% 
  group_by(closest_defender_name) %>% 
  summarise(shot_percentage = mean(shot_result == "made"),
            number_shots_defended = n()) %>% 
  ungroup() %>% 
  filter(number_shots_defended >= 100) %>% 
  arrange(shot_percentage)

```

## When you are defended by Lebron James, your shooting percentage tends to be below average.
```{r echo=T}

ggplot(df_best_defenders,
       aes(x = shot_percentage)) +
  geom_density() +
  geom_vline(xintercept = 0.44, linetype = "solid", colour = "red") +
  annotate("text", x = 0.45, y = 3, 
           label = paste("Lebron James, 325 shots defended"), 
           size = 4, angle = 90, colour = "red")

head(df_best_defenders)
```

## Correlations among predictors
There are some expected correlations among the predictors: 

* As shot distance increases,
so does the space a defender gives the shooting player. 
* Number of dribbles taken by the player who makes the shots and the amount of
time the player has had the ball in his hands are naturally highly correlated.
* The number of the shot and the number of the game period/quarter are also expected to be correlated.
```{r echo=T}
correlations <- cor(df_clean %>% select_if(is.numeric))
corrplot(correlations, order = "hclust")
```



# Model fitting preparation
## Data pre-processing
Due to a combination of constraints on computational power and size of the dataset, we'll only use 20 percent of the data as 
training data and the other 80% as testing data.

```{r echo=T}

# stratified random split of the data
df <- df_clean # only look at part of the data for exploratory analysis
in_training <- createDataPartition(y = df$shot_result, p = 0.2, list = FALSE)
df_train <- df[in_training, ]
df_test <- df[-in_training,]
```

## Parameter tuning options

We'll fit the models via 10-fold cross-validation. Since the 
problem is one of classification, we'll use the area under the 
receiver-operator-curve (ROC) as objective function to be maximised.
```{r echo=T}
# set up
fitted_models <- list()
model_predictions <- list()
fit_control <- trainControl(method = "cv", 
                            number = 10, 
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
pre_proc_options <- c("center", "scale")
optimisation_metric <- "ROC"

grids <- list()
model_specs <- list()
```


## Custom plotting functions to evluate model performance
```{r echo=T}
make_calibration_plot <- function(df, fitted_model) {
  df$probs <- predict.train(fitted_model, df, type = "prob")[,1]
  calibration_curve <- calibration(shot_result ~ probs, data = df)
  xyplot(calibration_curve, auto.key = list(columns = 2))
}

make_roc_plot <- function(df, fitted_model) {
  df$probs <- predict.train(fitted_model, df, type = "prob")[,2]
  roc_curve <- roc(response = df$shot_result, predictor = df$probs)
  print(auc(roc_curve))
  plot(roc_curve, legacy.axes = TRUE)
}
```


# Lasso Regression
We'll model shot success as a (linear) function of

* shot distance, 
* closest defender distance,
* time on the shot clock, 
* the final margin of the game (how close the game ends up being), 
* the touch time (how long the player has the ball before he takes the shot) and 
* whether the game is a home game or not.

The effect of shot distance and closest defender distance will be modelled by second-order
polynomials in order to allow effects to change non-linearly: the effect of distance may differ as distance increases.
The initial meter of extra space between the shooter and defender is likely more important
than an extra meter of space if the defender is already four meters away.

Additionally, we'll allow the effects of shot and defender distance to differ for
two and three point shots.

```{r echo=T}
model_specs$regression <- 
  shot_result ~ shot_dist*pts_type + 
  I(shot_dist^2) +
  I(shot_dist^2):pts_type + 
  closest_defender_dist*pts_type + 
  I(closest_defender_dist^2) +
  I(closest_defender_dist^2):pts_type +
  closest_defender_dist:shot_dist +
  shot_clock +
  final_margin +
  touch_time +
  dribbles +
  home_game +
  high_pct_scorer +
  low_pct_scorer +
  high_pct_defender +
  low_pct_defender 


tic()
grids$lasso <- expand.grid(alpha = 1,lambda = 10 ^ seq(-5, -3, length = 2))
set.seed(111)
fitted_models[["lasso"]] <- train(model_specs$regression, data = df_train,
                                  method = "glmnet",
                                  metric = optimisation_metric,
                                  preProc = pre_proc_options,
                                  trControl = fit_control,
                                  tuneGrid = grids[["lasso"]]
                                  )
model_predictions[["lasso"]] <- predict.train(fitted_models[["lasso"]], df_test)
toc()
```
It turns out that the best penalty value is the zero-penalty, indicating that the regression model
is not over-fitting.

```{r echo=T}
plot(fitted_models$lasso)
```

The model has an accuracy of 61%. It is better at identifying at misses, of which
it correctly identifies 76% (specificity), whereas only 43% of made shots are 
correctly identified (sensitivity).

```{r echo=T}
confusionMatrix(model_predictions$lasso,df_test$shot_result, positive = "made")
```

As expected, the most important variables are the shot and closest defender distance.

```{r echo=T}
varImp(fitted_models$lasso)

coef(fitted_models$lasso$finalModel, fitted_models$lasso$bestTune$lambda)
```

The calibration plot shows the predicted vs observed event(=success) rate over 
predicted event rate buckets of 10 percentage points.
In this case, this shows that there are no systematic under or overpredictions.

```{r echo=T}
make_calibration_plot(df_test, fitted_models$lasso)

make_roc_plot(df_test, fitted_models$lasso)
```



# Single Decision Tree

Moving on to less-parameterised methods, let's explore the results of fitting a single classification-tree model, before moving on to random forrests.

```{r echo=T}
model_specs$cart <- shot_result ~ shot_dist + pts_type + 
                                closest_defender_dist + 
                                shot_clock +
                                final_margin +
                                touch_time +
                                dribbles +
                                home_game

grids$cart <- expand.grid(cp = seq(0, 0.01, length = 10))

tic()
set.seed(111)
fitted_models[["cart"]] <- train(model_specs$cart, data = df_train,
                                  method = "rpart",
                                  metric = optimisation_metric,
                                  tuneGrid = grids$cart,
                                  trControl = fit_control
                                  )
model_predictions[["cart"]] <- predict.train(fitted_models[["cart"]], df_test)
toc()
```

As is typical, for low complexity parameter values, the model overfits, while for larger complexity parameter values and less complex models it underfits, resulting in an interior solution for the oprimal complexity parameter.
```{r echo=T}
plot(fitted_models[["cart"]])
```


```{r echo=T}
confusionMatrix(model_predictions[["cart"]],df_test$shot_result, positive = "made")
```

The calibration plot shows that the simple classification tree model, despite having a similar performance in terms of accuracy, has some severe cases of misclassification: certain pockets of the data that have a positive average-probability to be a succesful shot are predicted to have zero-likelihood of being succesful.  
```{r echo=T}
make_calibration_plot(df_test, fitted_models$cart)

make_roc_plot(df_test, fitted_models$cart)

```

```{r echo=T}
rpart.plot(fitted_models$cart$finalModel, type = 3, clip.right.lab = FALSE, branch = .3, under = TRUE)

```

# Random Forrests

To improve over the performance of the single-tree model, we will now average over 500 classification trees and, for each tree, randomly sample the variables that are used to build the best decision rule at each node. 

```{r echo=T}
model_specs$rf <- shot_result ~ shot_dist + pts_type + 
                                closest_defender_dist + 
                                shot_clock +
                                final_margin +
                                touch_time +
                                dribbles +
                                home_game

tic()
set.seed(111)
fitted_models[["rf"]] <- train(model_specs$rf, data = df_train,
                                  method = "ranger",
                                  metric = optimisation_metric,
                               tuneLength = 3,
                               trControl = fit_control
                                  )
model_predictions[["rf"]] <- predict.train(fitted_models[["rf"]], df_test)
toc()
```

```{r echo=T}
plot(fitted_models[["rf"]])
```

```{r echo=T}
confusionMatrix(model_predictions[["rf"]],df_test$shot_result, positive = "made")
```

```{r echo=T}
make_calibration_plot(df_test, fitted_models$rf)

make_roc_plot(df_test, fitted_models$rf)

```


# Comparing performance 

Finally, we compare the models' performance across the ten cross-validation splits,
to get a better understanding of the variability in the ROC numbers.

As it turns out, the logistic regression model trumps both the classification tree as well as the random forrests model. This indicates, that the underlying relationship between predictors and shot success is well covered by the specified linear structure of the logistic regression model. The comparatively low ROC numbers of all three models make more sense when taking into account the noisiness of the response: holding all factors equal, there is still a significant amount of uncertainty regarding the success of an individual basketball shot.
```{r echo=T}
results <- resamples(list(LogisticLasso = fitted_models[["lasso"]],
                          ClassificationTree = fitted_models[["cart"]], 
                          RandomForrests = fitted_models[["rf"]]
                          )
                     )
summary(results)
bwplot(results)
```



