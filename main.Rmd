---
title: "Predicting NBA shots"
output: rmarkdown::github_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE)
```
# Overview
Given the stochastic nature of basketball shots -- a given shot that was succesful may not be succesful again when repeated under the exact same match conditions, classifying NBA shots proves rather difficult. The model has particular difficulty to identify succesful shots.

TO DO

1. Enlarge feature space
    + Group data along the distances of the shot and the closest defender
    + Find players that outperform their peers in a given shot group
2. Refit Logistic Regression model plus MARS and Random Forrests


# Introduction
In this notebook, I classify basketball shots as succesful or unsuccesful shots.
Specifically, we'll look at all the basketball shots made during the 2014-2015 NBA 
season scraped from the NBA's API and provided on kaggle [(https://www.kaggle.com/dansbecker/nba-shot-logs/home)](https://www.kaggle.com/dansbecker/nba-shot-logs/home).

# Set up
```{r echo=T, results='hide', message=FALSE, warning=FALSE}
# clear workspace
remove(list = ls())
# clear console
cat("\014")

library(caret)
library(tidyverse)
library(skimr)
library(ggplot2)
library(tictoc)
library(corrplot)
library(pROC)
# for density plots
library(viridis) 
library(rpart.plot)
library(mlbench)
library(rgl)
library(MASS)
library(plot3D)

path_project <- "C:/Users/Mats Ole/Desktop/predicting_nba_shots/"
path_rel_data <- "data_input/"
name_data <- "shot_logs.csv"
```

# Data cleaning
First, let's clean and rename the data.
```{r}
df_raw <- read_csv(paste0(path_project, path_rel_data, name_data))

df_clean <- df_raw %>%
  transmute(game_id = as.factor(GAME_ID),
            matchup = as.factor(MATCHUP),
            home_game = case_when(LOCATION == "H" ~ TRUE,
                                  LOCATION == "A" ~ FALSE),
            win = case_when(W == "W" ~ TRUE,
                           W == "L" ~ FALSE),
            final_margin = abs(FINAL_MARGIN),
            shot_number = SHOT_NUMBER,
            period = PERIOD,
            game_clock = as.numeric(GAME_CLOCK),
            shot_clock = SHOT_CLOCK,
            dribbles = DRIBBLES,
            touch_time = TOUCH_TIME,
            shot_dist = SHOT_DIST,
            pts_type = as.factor(PTS_TYPE),
            shot_result = as.factor(SHOT_RESULT),
            closest_defender_name = as.factor(CLOSEST_DEFENDER),
            closest_defender_id = CLOSEST_DEFENDER_PLAYER_ID,
            closest_defender_dist = CLOSE_DEF_DIST,
            pts = PTS,
            player_name = as.factor(player_name),
            player_id = as.factor(player_id)
            ) %>% 
  dplyr::select(shot_result, everything())
```

Inspecting the data, we find that a few observations have missing shot clock
values. To keep things simple and given how few observations are concerned,
we'll drop them.
```{r, results='asis'}
df_clean <- 
  df_clean %>% drop_na()
```

# Data exploration

## Overall shot percentage
45.6% of the shots are succesful.
```{r echo=T}
mean(df_clean$shot_result == "made")
```
```{r echo=T}
options(scipen=999)
skim_with(numeric = list(hist = NULL))
kable(skim(df_clean))
```


## Most frequent shots are short-range 2-points shots or long-range 3-point shots.
```{r echo=T}

# Set display ranges to zoom in on areas where vast majority of data points lie 
display_range_x <- c(0, 30)
display_range_y <- c(0, 20)

ggplot(df_clean, 
       aes(x = shot_dist, 
           y = closest_defender_dist)
       ) +
  stat_density2d(aes(fill = ..density..), 
                 contour = F, geom = 'tile'
                 ) +
  scale_fill_viridis() + 
  ggtitle("Density distribution of all shots") +
  coord_cartesian(xlim = display_range_x, 
                  ylim = display_range_y
                  ) +
  geom_vline(xintercept = 22, 
             linetype = "dotted", 
             colour = "red") +
  geom_vline(xintercept = 23.75, 
             linetype = "dotted", 
             colour = "red") +
  geom_vline(xintercept = 23.75, 
             linetype = "dotted", 
             colour = "red") +
  annotate("text", 
           x = 21, 
           y = 15, 
           label = paste("Corner 3 points"), 
           size = 4, 
           angle = 90, 
           colour = "red") +
  annotate("text", 
           x = 24.75, 
           y = 15, 
           label = paste("Normal 3 points"), 
           size = 4, 
           angle = 90, 
           colour = "red")


```

## Short-range shots tend to be more succesful than long-range shots 
Below, I plot the difference in density between succesful and unsuccesful shots
in the shot-distance-closest-defender-space. The graph suggests that shot distance
and defender distance will play important rolls in classifying a shot succesful
or unsuccesful.
```{r}

df_made <- filter(df_clean, shot_result == "made")
df_missed <- filter(df_clean, shot_result == "missed")

n_grid <- 200

common_limits <- 
  c(range(df_clean$shot_dist), 
    range(df_clean$closest_defender_dist)
    )

kde_p <- kde2d(df_made$shot_dist,
               df_made$closest_defender_dist,
               n = n_grid, 
               lims = common_limits)

kde_n <- kde2d(df_missed$shot_dist,
               df_missed$closest_defender_dist,
               n = n_grid, 
               lims = common_limits)

z <- kde_p$z - kde_n$z

image2D(x = kde_p$x, 
        y = kde_p$y, 
        z,
        col = RColorBrewer::brewer.pal(11,"Spectral"),
        xlab = "Shot Distance",
        ylab = "Defender Distance",
        clab = "Density difference",
        shade = 0, 
        rasterImage = TRUE, 
        xlim = display_range_x, 
        ylim = display_range_y,
        contour = list(col = "white", 
                       labcex = 0.8, 
                       lwd = 1, 
                       alpha = 0.5)
        )


```



# Model fitting preparation
## Data pre-processing
We'll do a typical 80/20 training-test split, making sure that proportions of succesful shots are similar across the two samples.

```{r echo=T}

# stratified random split of the data
df <- df_clean # only look at part of the data for exploratory analysis
in_training <- createDataPartition(y = df$shot_result, p = 0.8, list = FALSE)
df_train <- df[in_training, ]
df_test <- df[-in_training,]
```

## Parameter tuning options

When fitting the models, we'll use via 3-fold cross-validation to tune the models and to get a first approximation to their out-of-training-sample performance. Since we are trying to classify shots as succesful or unsuccesful, we'll use the area under the receiver-operator-curve (ROC) as objective function to be maximised. Carets option to center and scales predictors will also be used.
```{r echo=T}
# set up
fitted_models <- list()
model_predictions <- list()
fit_control <- trainControl(method = "cv", 
                            number = 3, 
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
pre_proc_options <- c("center", "scale")
optimisation_metric <- "ROC"

grids <- list()
model_specs <- list()
```


## Custom plotting functions to evluate model performance
```{r echo=T}
make_calibration_plot <- function(df, fitted_model) {
  df$probs <- predict.train(fitted_model, df, type = "prob")[,1]
  calibration_curve <- calibration(shot_result ~ probs, data = df)
  xyplot(calibration_curve, auto.key = list(columns = 2))
}

make_roc_plot <- function(df, fitted_model) {
  df$probs <- predict.train(fitted_model, df, type = "prob")[,2]
  roc_curve <- roc(response = df$shot_result, predictor = df$probs)
  print(auc(roc_curve))
  plot(roc_curve, legacy.axes = TRUE)
}
```


# Lasso Regression
We'll model shot success as a (linear) function of

* shot distance, 
* closest defender distance,
* time on the shot clock, 
* the final margin of the game (how close the game ends up being), 
* the touch time (how long the player has the ball before he takes the shot) and 
* whether the game is a home game or not.

The effect of shot distance and closest defender distance will be modelled by second-order
polynomials in order to allow effects to change non-linearly: the effect of distance may differ as distance increases.
The initial meter of extra space between the shooter and defender is likely more important
than an extra meter of space if the defender is already four meters away.

Additionally, we'll allow the effects of shot and defender distance to differ for
two and three point shots.

```{r echo=T}

model_specs$regression <- 
  shot_result ~ shot_dist*pts_type + 
  I(shot_dist^2) +
  I(shot_dist^2):pts_type + 
  closest_defender_dist*pts_type + 
  I(closest_defender_dist^2) +
  I(closest_defender_dist^2):pts_type +
  closest_defender_dist:shot_dist +
  shot_clock +
  final_margin +
  touch_time +
  dribbles +
  home_game


tic()
grids$lasso <- expand.grid(alpha = 1,lambda = 10 ^ seq(-5, -1, length = 5))
set.seed(111)
fitted_models[["lasso"]] <- train(model_specs$regression, data = df_train,
                                  method = "glmnet",
                                  metric = optimisation_metric,
                                  preProc = pre_proc_options,
                                  trControl = fit_control,
                                  tuneGrid = grids[["lasso"]]
                                  )
model_predictions[["lasso"]] <- predict.train(fitted_models[["lasso"]], df_test)
toc()
```
It turns out that the best penalty value is the zero-penalty, indicating that the regression model
is not over-fitting.

```{r echo=T}
plot(fitted_models$lasso)
```

The model has an accuracy of 61%. It is better at identifying at misses, of which
it correctly identifies 76% (specificity), whereas only 43% of made shots are 
correctly identified (sensitivity).

```{r echo=T}
confusionMatrix(model_predictions$lasso,df_test$shot_result, positive = "made")
```

As expected, the most important variables are the distance of the shot and the distance to the closest defender.

```{r echo=T}
varImp(fitted_models$lasso)

coef(fitted_models$lasso$finalModel, fitted_models$lasso$bestTune$lambda)
```


Comparing the predictions of the model with the test data, the achieved ROC is 0.6275.

```{r echo=T}
make_roc_plot(df_test, fitted_models$lasso)
```

# Performance of non-parametric methods
It turns out, that the performance of non-parametric methods such as MARS or random forrests, is not siginificantly better than the base model above.
To improve performance, let's create additional features.




# Generating additional features -- leveraging individual player info
The low ROC of the model is largely due to its failure to correclty identify succesful shots, as indicated by the low sensitivity of the model. The strategy to remedy this will be to enlarge the feature space. Specificly, we will sort shots into groups within the shot-distance-closest-defender-distance space and try to identify players, which score better than the rest in a given distance group. This should improve the result more than simply adding individual player dummies, as different players have different "sweet spots" conditions on court under which they excel in comparison to their peers.

The enlarged feature space should also boost the performance of non-parametric models like MARS and random forrets.

# TO DO

1. Generate dummies for different areas in shot-distance-closest-defender-distance space
2. Fit simple OLS model and look at residuals to identify players that outperform the rest
3. Refit Logistic Lasso, RF and MARS models


